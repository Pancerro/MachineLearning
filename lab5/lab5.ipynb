{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjCU5v9ROeGg4wdsszxfQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pancerro/MachineLearning/blob/master/lab5/lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-vcFmO_BS82K",
        "outputId": "945b6781-26c0-407b-f05e-75b1d39d203d"
      },
      "source": [
        "import spacy\n",
        "#Jeśli nlp obiekt został utworzony, oznacza to, że zainstalowano spaCy i pomyślnie pobrano modele i dane.\n",
        "nlp = spacy.load(\"pl_core_news_sm\")\n",
        "print(nlp)\n",
        "from google.colab import files\n",
        "\n",
        "introduction_text = ('To jest po polsku' ' To tak jest czasem ze jest po polsku.')\n",
        "introduction_doc = nlp(introduction_text)\n",
        "#tekst jest konwertowany na obiekt, który jest rozumiany przez spaCy. Możesz użyć tej metody, aby przekonwertować dowolny tekst na przetworzony Docobiekt i wydedukować atrybuty\n",
        "print([token.text for token in introduction_doc])\n",
        "\n",
        "#Przekowertowanie pliku .text na obiekt doc\n",
        "#uploaded = files.upload()\n",
        "#introduction_file_doc = nlp(uploaded)\n",
        "#print([token.text for token in introduction_file_doc])\n",
        "\n",
        "#Wykrywanie zdan\n",
        "about_text = ('ty Adrian Ławecki Pierwsze zdanie - jest długie a'\n",
        "             ' drugie jest o wiele krótsze i niepotrzebne'\n",
        "             ' trzeba je usunąć. Interesuje sie tymi zdaniami'\n",
        "             ' bo są Pierwsze ciekawe.')\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents)\n",
        "len(sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "     print (sentence)\n",
        "\n",
        "\n",
        "#wykrywanie zdan z customowym separatorem\n",
        "def set_custom_boundaries(doc):\n",
        "     for token in doc[:-1]:\n",
        "         if token.text == '...':\n",
        "             doc[token.i+1].is_sent_start = True\n",
        "     return doc\n",
        "\n",
        "ellipsis_text = ('ty Adrian Ławecki Pierwsze zdanie - jest długie a'\n",
        "             ' drugie jest o wiele krótsze i niepotrzebne'\n",
        "             ' trzeba je usunąć... Interesuje... sie tymi zdaniami'\n",
        "             ' bo są ciekawe...')\n",
        "custom_nlp = spacy.load('pl_core_news_sm')\n",
        "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
        "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
        "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
        "for sentence in custom_ellipsis_sentences:\n",
        "    print(sentence)\n",
        "ellipsis_doc = nlp(ellipsis_text)\n",
        "ellipsis_sentences = list(ellipsis_doc.sents)\n",
        "for sentence in ellipsis_sentences:\n",
        "    print(sentence)\n",
        "\n",
        "#W spaCy możesz wypisywac tokeny, wykonując iterację na Docobiekcie:\n",
        "for token in about_doc:\n",
        "    print (token, token.idx)\n",
        "#tokeny zawieraja takze rozne atrybuty takie jak np: \n",
        "#text_with_ws drukuje tekst tokena ze spacją na końcu (jeśli występuje).\n",
        "#is_alpha wykrywa, czy token składa się ze znaków alfabetu, czy nie.\n",
        "#is_punct wykrywa, czy token jest symbolem interpunkcyjnym, czy nie.\n",
        "#is_space wykrywa, czy żeton jest spacją, czy nie.\n",
        "#shape_ wypisuje kształt słowa.\n",
        "#is_stop wykrywa, czy token jest słowem stop, czy nie.\n",
        "for token in about_doc:\n",
        "    print(token, token.idx, token.text_with_ws,\n",
        "          token.is_alpha, token.is_punct, token.is_space,\n",
        "          token.shape_, token.is_stop)\n",
        "#Dostosowanie proces tokenizacji, aby wykrywał tokeny na niestandardowych postaciach\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import re\n",
        "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
        "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
        "infix_re = re.compile(r'''[-~]''')\n",
        "def customize_tokenizer(nlp):\n",
        "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
        "                     suffix_search=suffix_re.search,\n",
        "                     infix_finditer=infix_re.finditer,\n",
        "                     token_match=None\n",
        "                    )\n",
        "\n",
        "\n",
        "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n",
        "custom_tokenizer_about_doc = custom_nlp(about_text)\n",
        "print([token.text for token in custom_tokenizer_about_doc])\n",
        "\n",
        "#Lista słow pomijania - slowa ktora nie maja znaczenia i mozna pominac\n",
        "spacy_stopwords = spacy.lang.pl.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)\n",
        "\n",
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        "    print(stop_word)\n",
        "#usuneicie slow pomijanych z tekstu\n",
        "for token in about_doc:\n",
        "    if not token.is_stop:\n",
        "     print(token)\n",
        "#Lista slow bez slow pomijanych\n",
        "about_no_stopword_doc = [token for token in about_doc if not token.is_stop]\n",
        "print(about_no_stopword_doc)\n",
        "#uzycie lementyzacji \n",
        "conference_help_doc = nlp(about_text)\n",
        "for token in conference_help_doc:\n",
        "    print(token, token.lemma_)\n",
        "#analiza teskotw - unikalne slowa oraz liczba slow ktore sie powtarzaja\n",
        "from collections import Counter\n",
        "complete_doc = nlp(about_text)\n",
        "words = [token.text for token in complete_doc\n",
        "         if not token.is_stop and not token.is_punct]\n",
        "word_freq = Counter(words)\n",
        "common_words = word_freq.most_common(5)\n",
        "print(common_words)\n",
        "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
        "print(unique_words)\n",
        "#To samo ale z slowami pomijanymi\n",
        "words_all = [token.text for token in complete_doc if not token.is_punct]\n",
        "word_freq_all = Counter(words_all)\n",
        "common_words_all = word_freq_all.most_common(5)\n",
        "print(common_words_all)\n",
        "#Tekst z funkcja jaka pelni w tekscie\n",
        "for token in about_doc:\n",
        "    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
        "\n",
        "#Wyodrobiona kategoria slow\n",
        "nouns = []\n",
        "adjectives = []\n",
        "for token in about_doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "       nouns.append(token)\n",
        "    if token.pos_ == 'ADJ':\n",
        "       adjectives.append(token)\n",
        "\n",
        "#wizualizacja analizy zależności lub nazwanych encji\n",
        "#from spacy import displacy\n",
        "#about_interest_text = ('Jest to interesujace do nauki' \n",
        "#                       ' Naturalny językowy proces.')\n",
        "#about_interest_doc = nlp(about_interest_text)\n",
        "#displacy.serve(about_interest_doc, style='dep')\n",
        "\n",
        "\n",
        "\n",
        "#Funkcja przetwarzania wstępnego konwertuje tekst na format możliwy do analizy\n",
        "#Zmniejsza tekst\n",
        "#Lemmatyzuje każdy token\n",
        "#Usuwa symbole interpunkcyjne\n",
        "#Usuwa pomijane słowa\n",
        "def is_token_allowed(token):\n",
        "     if (not token or not token.string.strip() or\n",
        "         token.is_stop or token.is_punct):\n",
        "         return False\n",
        "     return True\n",
        "def preprocess_token(token):\n",
        "    return token.lemma_.strip().lower()\n",
        "complete_filtered_tokens = [preprocess_token(token)\n",
        "                            for token in complete_doc if is_token_allowed(token)]\n",
        "complete_filtered_tokens\n",
        "\n",
        "#Za pomoca regul proba wyodrebienia imiai nazwiska za pomoca reguł\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "def extract_full_name(nlp_doc):\n",
        "  pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "  matcher.add('FULL_NAME', None, pattern)\n",
        "  matches = matcher(nlp_doc)\n",
        "  for match_id, start, end in matches:\n",
        "    span = nlp_doc[start:end]\n",
        "    return span.text\n",
        "print(extract_full_name(about_doc))\n",
        "#szukanie numeru telefonu\n",
        "matcher = Matcher(nlp.vocab)\n",
        "conference_org_text = ('Zrobie na zajecia'\n",
        "  ' Poszukiwanie numeru telefonicznego '\n",
        "  ' \"Szukaj numeru\".'\n",
        "  ' Zostaw swój number'\n",
        "  ' numer 506 658 535')\n",
        "def extract_phone_number(nlp_doc):\n",
        "  pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n",
        "             {'ORTH': ')'}, {'SHAPE': 'ddd'},\n",
        "             {'ORTH': '-', 'OP': '?'},\n",
        "             {'SHAPE': 'ddd'}]\n",
        "  matcher.add('PHONE_NUMBER', None, pattern)\n",
        "  matches = matcher(nlp_doc)\n",
        "  for match_id, start, end in matches:\n",
        "    span = nlp_doc[start:end]\n",
        "    return span.text\n",
        "conference_org_doc = nlp(conference_org_text)\n",
        "print(extract_phone_number(conference_org_doc))\n",
        "\n",
        "#Analiza tekstu szukajac zaleznosci i powiazan w tekscie\n",
        "piano_text = 'Marta uczy się gry na pianinie.'\n",
        "piano_doc = nlp(piano_text)\n",
        "for token in piano_doc:\n",
        "  print(token.text, token.tag_, token.head.text, token.dep_)\n",
        "\n",
        "#displacy.serve(piano_doc, style='dep')\n",
        "#Przegladane tekstu i zaleznosci z uzyciem drzewa\n",
        "one_line_about_text = ('Adrian Ławecki czuje się dzisiaj dobrze'\n",
        "  ' aktualnie studiuje w Akademi Marynarki Wojennej')\n",
        "one_line_about_doc = nlp(one_line_about_text)\n",
        "print([token.text for token in one_line_about_doc[2].children])\n",
        "print(one_line_about_doc[5].nbor(-1))\n",
        "print(one_line_about_doc[5].nbor())\n",
        "print([token.text for token in one_line_about_doc[2].lefts])\n",
        "print([token.text for token in one_line_about_doc[2].rights])\n",
        "print(list(one_line_about_doc[5].subtree))\n",
        "#zwraca ciąg, scalając zawarte w nim słowa\n",
        "def flatten_tree(tree):\n",
        "  return ''.join([token.text_with_ws for token in list(tree)]).strip()\n",
        "print(flatten_tree(one_line_about_doc[5].subtree))\n",
        "\n",
        "\n",
        "#Pomaga ci wywnioskować, o czym jest mowa w zdaniu.\n",
        "conference_text = ('Jest tu organizowana konferencja'\n",
        "  ' Odbywa się ona 21 Czerwca 2019 w Gdyni.')\n",
        "conference_doc = nlp(conference_text)\n",
        "for chunk in conference_doc.noun_chunks:\n",
        "  print(chunk)\n",
        "\n",
        "#wyodrębnienia fraz czasowników\n",
        "#!pip install textacy\n",
        "import textacy\n",
        "about_talk_text = ('Będziemy sobie gadać o sztucznej inteligencji'\n",
        "  ' i naturalnym procesie nauki'\n",
        "  ' Jest fajnie')\n",
        "pattern = r'(<VERB>?<ADV>*<VERB>+)'\n",
        "about_talk_doc = textacy.make_spacy_doc(about_talk_text,\n",
        "                                        lang='en_core_web_sm')\n",
        "verb_phrases = textacy.extract.pos_regex_matches(about_talk_doc, pattern)\n",
        "\n",
        "for chunk in verb_phrases:\n",
        "  print(chunk.text)\n",
        "\n",
        "for chunk in about_talk_doc.noun_chunks:\n",
        "  print(chunk)\n",
        "\n",
        "# wyodrębnienia nazwanych jednostek:\n",
        "piano_class_text = ('Najlepsza akademia pianistyczna'\n",
        "  ' Jest w takim mieście co Londyn się nazywa'\n",
        "  ' jest tam klasa prowadzana przez światowych in.')\n",
        "piano_class_doc = nlp(piano_class_text)\n",
        "for ent in piano_class_doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char,\n",
        "        ent.label_, spacy.explain(ent.label_))\n",
        "#displacy.serve(piano_class_doc, style='ent')\n",
        "# użyć NER, aby wymazać nazwiska osób z tekstu\n",
        "survey_text = ('Spośród 5 ankietowanych osób, James Robert,'\n",
        "               ' „Julie Fuller i Benjamin Brooks lubią”  jabłka.' \n",
        "               ' Kelly Cox i Matthew Evans  „jak pomarańcze')\n",
        "\n",
        "def replace_person_names(token):\n",
        "  if token.ent_iob != 0 and token.ent_type_ == 'PERSON':\n",
        "     return '[REDACTED] '\n",
        "     return token.string\n",
        "\n",
        "def redact_names(nlp_doc):\n",
        "  for ent in nlp_doc.ents:\n",
        "     ent.merge()\n",
        "     tokens = map(replace_person_names, nlp_doc)\n",
        "     return ''.join(tokens)\n",
        "survey_doc = nlp(survey_text)\n",
        "redact_names(survey_doc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<spacy.lang.pl.Polish object at 0x7fc1cee6a4a8>\n",
            "['To', 'jest', 'po', 'polsku', 'To', 'tak', 'jest', 'czasem', 'ze', 'jest', 'po', 'polsku', '.']\n",
            "ty Adrian Ławecki Pierwsze zdanie - jest długie a drugie jest o wiele krótsze i niepotrzebne trzeba je usunąć.\n",
            "Interesuje sie tymi zdaniami bo są Pierwsze ciekawe.\n",
            "ty Adrian Ławecki Pierwsze zdanie - jest długie a drugie jest o wiele krótsze i niepotrzebne trzeba je usunąć...\n",
            "Interesuje...\n",
            "sie tymi zdaniami bo są ciekawe...\n",
            "ty Adrian Ławecki Pierwsze zdanie - jest długie a drugie jest o wiele krótsze i niepotrzebne trzeba je usunąć...\n",
            "Interesuje...\n",
            "sie tymi zdaniami bo są ciekawe...\n",
            "ty 0\n",
            "Adrian 3\n",
            "Ławecki 10\n",
            "Pierwsze 18\n",
            "zdanie 27\n",
            "- 34\n",
            "jest 36\n",
            "długie 41\n",
            "a 48\n",
            "drugie 50\n",
            "jest 57\n",
            "o 62\n",
            "wiele 64\n",
            "krótsze 70\n",
            "i 78\n",
            "niepotrzebne 80\n",
            "trzeba 93\n",
            "je 100\n",
            "usunąć 103\n",
            ". 109\n",
            "Interesuje 111\n",
            "sie 122\n",
            "tymi 126\n",
            "zdaniami 131\n",
            "bo 140\n",
            "są 143\n",
            "Pierwsze 146\n",
            "ciekawe 155\n",
            ". 162\n",
            "ty 0 ty  True False False xx True\n",
            "Adrian 3 Adrian  True False False Xxxxx False\n",
            "Ławecki 10 Ławecki  True False False Xxxxx False\n",
            "Pierwsze 18 Pierwsze  True False False Xxxxx False\n",
            "zdanie 27 zdanie  True False False xxxx False\n",
            "- 34 -  False True False - False\n",
            "jest 36 jest  True False False xxxx True\n",
            "długie 41 długie  True False False xxxx False\n",
            "a 48 a  True False False x True\n",
            "drugie 50 drugie  True False False xxxx False\n",
            "jest 57 jest  True False False xxxx True\n",
            "o 62 o  True False False x True\n",
            "wiele 64 wiele  True False False xxxx True\n",
            "krótsze 70 krótsze  True False False xxxx False\n",
            "i 78 i  True False False x True\n",
            "niepotrzebne 80 niepotrzebne  True False False xxxx False\n",
            "trzeba 93 trzeba  True False False xxxx True\n",
            "je 100 je  True False False xx True\n",
            "usunąć 103 usunąć True False False xxxx False\n",
            ". 109 .  False True False . False\n",
            "Interesuje 111 Interesuje  True False False Xxxxx False\n",
            "sie 122 sie  True False False xxx True\n",
            "tymi 126 tymi  True False False xxxx False\n",
            "zdaniami 131 zdaniami  True False False xxxx False\n",
            "bo 140 bo  True False False xx True\n",
            "są 143 są  True False False xx True\n",
            "Pierwsze 146 Pierwsze  True False False Xxxxx False\n",
            "ciekawe 155 ciekawe True False False xxxx False\n",
            ". 162 . False True False . False\n",
            "['ty', 'Adrian', 'Ławecki', 'Pierwsze', 'zdanie', '-', 'jest', 'długie', 'a', 'drugie', 'jest', 'o', 'wiele', 'krótsze', 'i', 'niepotrzebne', 'trzeba', 'je', 'usunąć', '.', 'Interesuje', 'sie', 'tymi', 'zdaniami', 'bo', 'są', 'Pierwsze', 'ciekawe', '.']\n",
            "także\n",
            "powinien\n",
            "cały\n",
            "właśnie\n",
            "iż\n",
            "będą\n",
            "tego\n",
            "że\n",
            "wśród\n",
            "albo\n",
            "Adrian\n",
            "Ławecki\n",
            "Pierwsze\n",
            "zdanie\n",
            "-\n",
            "długie\n",
            "drugie\n",
            "krótsze\n",
            "niepotrzebne\n",
            "usunąć\n",
            ".\n",
            "Interesuje\n",
            "tymi\n",
            "zdaniami\n",
            "Pierwsze\n",
            "ciekawe\n",
            ".\n",
            "[Adrian, Ławecki, Pierwsze, zdanie, -, długie, drugie, krótsze, niepotrzebne, usunąć, ., Interesuje, tymi, zdaniami, Pierwsze, ciekawe, .]\n",
            "ty ty\n",
            "Adrian Adriana\n",
            "Ławecki ławecki\n",
            "Pierwsze pierwszy\n",
            "zdanie zdanie\n",
            "- -\n",
            "jest być\n",
            "długie długi\n",
            "a a\n",
            "drugie drugi\n",
            "jest być\n",
            "o o\n",
            "wiele wiele\n",
            "krótsze krótki\n",
            "i i\n",
            "niepotrzebne potrzebny\n",
            "trzeba trzeba\n",
            "je on\n",
            "usunąć usunąć\n",
            ". .\n",
            "Interesuje interesować\n",
            "sie sie\n",
            "tymi ten\n",
            "zdaniami zdanie\n",
            "bo bo\n",
            "są być\n",
            "Pierwsze pierwszy\n",
            "ciekawe ciekawy\n",
            ". .\n",
            "[('Pierwsze', 2), ('Adrian', 1), ('Ławecki', 1), ('zdanie', 1), ('długie', 1)]\n",
            "['Adrian', 'Ławecki', 'zdanie', 'długie', 'drugie', 'krótsze', 'niepotrzebne', 'usunąć', 'Interesuje', 'tymi', 'zdaniami', 'ciekawe']\n",
            "[('Pierwsze', 2), ('jest', 2), ('ty', 1), ('Adrian', 1), ('Ławecki', 1)]\n",
            "ty PPRON12 PRON None\n",
            "Adrian SUBST NOUN None\n",
            "Ławecki SUBST NOUN None\n",
            "Pierwsze ADJ ADJ adjective\n",
            "zdanie SUBST NOUN None\n",
            "- INTERP PUNCT None\n",
            "jest FIN VERB None\n",
            "długie ADJ ADJ adjective\n",
            "a CONJ CCONJ conjunction\n",
            "drugie ADJ ADJ adjective\n",
            "jest FIN VERB None\n",
            "o PREP ADP None\n",
            "wiele ADV ADV adverb\n",
            "krótsze ADJ ADJ adjective\n",
            "i CONJ CCONJ conjunction\n",
            "niepotrzebne ADJ ADJ adjective\n",
            "trzeba PRED VERB None\n",
            "je PPRON3 PRON None\n",
            "usunąć INF VERB None\n",
            ". INTERP PUNCT None\n",
            "Interesuje FIN VERB None\n",
            "sie QUB PART None\n",
            "tymi ADJ ADJ adjective\n",
            "zdaniami SUBST NOUN None\n",
            "bo COMP SCONJ None\n",
            "są FIN VERB None\n",
            "Pierwsze ADJ ADJ adjective\n",
            "ciekawe ADJ ADJ adjective\n",
            ". INTERP PUNCT None\n",
            "None\n",
            "None\n",
            "Marta SUBST uczy nsubj\n",
            "uczy FIN uczy ROOT\n",
            "się QUB uczy expl:pv\n",
            "gry SUBST uczy obj\n",
            "na PREP pianinie case\n",
            "pianinie SUBST gry nmod\n",
            ". INTERP uczy punct\n",
            "['Adrian', 'się', 'dzisiaj', 'studiuje']\n",
            "dzisiaj\n",
            "aktualnie\n",
            "['Adrian']\n",
            "['się', 'dzisiaj', 'studiuje']\n",
            "[dobrze]\n",
            "dobrze\n",
            "Będziemy\n",
            "i naturalnym procesie\n",
            "Jest fajnie\n",
            "Londyn 56 62 persName None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/textacy/extract.py:338: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
            "  action=\"once\",\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:253: DeprecationWarning: [W013] As of v2.1.0, Span.merge is deprecated. Please use the more efficient and less error-prone Doc.retokenize context manager instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:253: DeprecationWarning: [W013] As of v2.1.0, Doc.merge is deprecated. Please use the more efficient and less error-prone Doc.retokenize context manager instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-85e70674e56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0msurvey_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurvey_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m \u001b[0mredact_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurvey_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-85e70674e56b>\u001b[0m in \u001b[0;36mredact_names\u001b[0;34m(nlp_doc)\u001b[0m\n\u001b[1;32m    253\u001b[0m      \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m      \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace_person_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m      \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0msurvey_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurvey_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0mredact_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurvey_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
          ]
        }
      ]
    }
  ]
}